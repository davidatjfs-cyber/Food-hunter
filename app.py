import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- 1. é¡µé¢é…ç½® (æ¢æˆç¾é£Ÿä¸»é¢˜) ---
st.set_page_config(page_title="FoodHunter - é¤é¥®æƒ…æŠ¥å®˜", page_icon="ğŸ¦", layout="wide")

st.title("ğŸ¦ FoodHunter: æ‚¨çš„ AI é¤é¥®ç ”å‘æ€»ç›‘")
st.markdown("### ä¸“æŸ¥ï¼šçˆ†æ¬¾èœå“ / æµè¡Œå£å‘³ / è¥é”€çµæ„Ÿ / ç«å“åˆ†æ")

# --- 2. ä¾§è¾¹æ  (ä¿æŒä¸å˜ï¼Œæ–¹ä¾¿ä½ ç›´æ¥ç”¨) ---
with st.sidebar:
    st.header("ğŸ”‘ ç³»ç»Ÿé…ç½®")
    
    # é»˜è®¤é€‰ DeepSeekï¼Œå› ä¸ºä½ å·²ç»å……å€¼äº†
    provider = st.selectbox("é€‰æ‹©æ¨¡å‹å‚å•†", ["DeepSeek (æ·±åº¦æ±‚ç´¢)", "OpenAI", "Moonshot (Kimi)"])
    
    if provider == "OpenAI":
        base_url = "https://api.openai.com/v1"
        model_name = "gpt-4o"
    elif provider == "DeepSeek (æ·±åº¦æ±‚ç´¢)":
        base_url = "https://api.deepseek.com"
        model_name = "deepseek-chat" 
    elif provider == "Moonshot (Kimi)":
        base_url = "https://api.moonshot.cn/v1"
        model_name = "moonshot-v1-8k"

    # è¿™é‡Œæé†’ç”¨æˆ·å¡« Key
    llm_api_key = st.text_input("å¤§æ¨¡å‹ API Key", type="password", help="æ¨èä½¿ç”¨ DeepSeek")
    tavily_api_key = st.text_input("Tavily API Key", type="password", help="æœç´¢ä¸“ç”¨")

# --- 3. æ ¸å¿ƒ Prompt (è¿™æ˜¯æœ¬æ¬¡æ”¹é€ çš„çµé­‚ï¼) ---
# æˆ‘ä»¬æŠŠâ€œå®¡è®¡å¸ˆâ€æ¢æˆäº†â€œé¤é¥®ç ”å‘æ€»ç›‘â€
TREND_HUNTER_PROMPT = """
ä½ æ˜¯ä¸€åæ‹¥æœ‰15å¹´ç»éªŒçš„ã€é¤é¥®ç ”å‘æ€»ç›‘ã€‘å…¼ã€å“ç‰Œè¥é”€ä¸“å®¶ã€‘ã€‚
ä½ ç†Ÿæ‚‰ä¸­å›½é¤é¥®å¸‚åœºï¼Œæ“…é•¿é€šè¿‡ç½‘ç»œæ•°æ®æŒ–æ˜æœ€æ–°çš„ã€çˆ†æ¬¾èœå“ã€‘ã€ã€æµè¡Œå£å‘³ã€‘å’Œã€è¥é”€ç©æ³•ã€‘ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åŸºäºã€æœç´¢ç»“æœã€‘ï¼Œå›ç­”è€æ¿çš„è°ƒç ”éœ€æ±‚ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„è¾“å‡ºç­–åˆ’æ¡ˆï¼š

# ğŸ’¡ é¤é¥®æƒ…æŠ¥åˆ†ææŠ¥å‘Š

### 1. ğŸ¯ æ ¸å¿ƒè¶‹åŠ¿æç‚¼
(ç”¨ä¸€å¥è¯æ€»ç»“ç›®å‰çš„å¸‚åœºçƒ­ç‚¹ï¼Œä¾‹å¦‚ï¼š"è„†çš®äº”èŠ±è‚‰æ­£åœ¨å¤œå¸‚å’ŒæŠ–éŸ³çˆ†ç«ï¼Œæ ¸å¿ƒåœ¨äºå¬è§‰åˆºæ¿€")

### 2. ğŸ² çˆ†æ¬¾æ‹†è§£ (What & Why)
* **æµè¡Œäº§å“/å£å‘³ï¼š** (å…·ä½“æ˜¯ä»€ä¹ˆèœï¼Ÿä»€ä¹ˆæ­é…ï¼Ÿä¾‹å¦‚ï¼šç«é”…+å¥¶èŒ¶)
* **ç«çˆ†é€»è¾‘ï¼š** (ä¸ºä»€ä¹ˆå¹´è½»äººå–œæ¬¢ï¼Ÿæ˜¯æ‹ç…§å¥½çœ‹ï¼Ÿæ€§ä»·æ¯”é«˜ï¼Ÿè¿˜æ˜¯å£å‘³çŒå¥‡ï¼Ÿ)
* **å…¸å‹æ¡ˆä¾‹ï¼š** (æœç´¢ç»“æœä¸­æåˆ°çš„åšå¾—å¥½çš„å“ç‰Œæˆ–åº—é“º)

### 3. ğŸ› ï¸ è½åœ°å»ºè®® (Action Plan)
* **å¦‚æœä¸æ¢èœå•ï¼š** (å¦‚ä½•ç”¨ç°æœ‰é£Ÿæå¾®è°ƒæ¥è¹­çƒ­ç‚¹ï¼Ÿ)
* **å¦‚æœæ¨æ–°å“ï¼š** (ç»™å‡ºä¸€ä¸ªå…·ä½“çš„æ–°èœåå’Œç®€å•çš„åšæ³•/æ‘†ç›˜å»ºè®®)
* **è¥é”€è¯æœ¯ï¼š** (å†™ä¸€å¥å‘æœ‹å‹åœˆ/æŠ–éŸ³çš„æ–‡æ¡ˆï¼Œè¦å¸å¼•äºº)

---
**æ•°æ®æ¥æºï¼š** {evidence}
"""

# --- 4. ä¸»é€»è¾‘ ---
# ä¿®æ”¹äº†ç¤ºä¾‹é—®é¢˜
user_input = st.text_area("ä½ æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ", height=100, 
                         placeholder="ä¾‹å¦‚ï¼š\n1. æœ€è¿‘ç«é”…åº—æœ‰ä»€ä¹ˆæ–°çš„ç”œå“çˆ†æ¬¾ï¼Ÿ\n2. ç°åœ¨çš„å¹´è½»äººå–œæ¬¢åƒä»€ä¹ˆå£å‘³çš„çƒ¤é±¼ï¼Ÿ\n3. å¸®æˆ‘æŸ¥æŸ¥â€˜åŠ ä¸Šå¤´â€™è¿™å®¶åº—ä¸ºä»€ä¹ˆç«ï¼Ÿ")

check_btn = st.button("ğŸ” å¼€å§‹æŒ–æ˜çµæ„Ÿ", type="primary")

if check_btn:
    if not llm_api_key or not tavily_api_key:
        st.error("âŒ åˆ«å¿˜äº†åœ¨å·¦ä¾§å¡«å…¥ API Keys (DeepSeek å’Œ Tavily)")
    else:
        try:
            with st.status("ğŸ‘¨â€ğŸ³ æ­£åœ¨å…¨ç½‘æœç½—ç¾é£Ÿæƒ…æŠ¥...", expanded=True) as status:
                
                # 1. æœç´¢
                status.write("æ­£åœ¨æ£€ç´¢å°çº¢ä¹¦/å¤§ä¼—ç‚¹è¯„/æŠ–éŸ³çš„æµè¡Œè¶‹åŠ¿ (via Tavily)...")
                search = TavilySearchResults(tavily_api_key=tavily_api_key, max_results=5)
                # è‡ªåŠ¨åœ¨æœç´¢è¯ååŠ ä¸Šâ€œè¶‹åŠ¿â€ã€â€œçˆ†æ¬¾â€ç­‰è¯ï¼Œæé«˜æœç´¢è´¨é‡
                query = f"{user_input} æœ€æ–°é¤é¥®è¶‹åŠ¿ çˆ†æ¬¾"
                evidence = search.invoke(query)
                status.write(f"âœ… é‡‡é›†åˆ° {len(evidence)} æ¡å¸‚åœºæƒ…æŠ¥")
                
                # 2. æ¨ç†
                status.write(f"ç ”å‘æ€»ç›‘ ({provider}) æ­£åœ¨æ’°å†™ç­–åˆ’æ¡ˆ...")
                llm = ChatOpenAI(
                    base_url=base_url,
                    api_key=llm_api_key,
                    model=model_name,
                    temperature=0.7 # ç¨å¾®è°ƒé«˜ä¸€ç‚¹ï¼Œè®© AI æ›´æœ‰åˆ›æ„
                )
                
                chain = ChatPromptTemplate.from_messages([
                    ("system", TREND_HUNTER_PROMPT),
                    ("user", "è€æ¿çš„éœ€æ±‚: {input}\n\nå¸‚åœºæƒ…æŠ¥: {evidence}")
                ]) | llm | StrOutputParser()
                
                report = chain.invoke({"input": user_input, "evidence": evidence})
                status.update(label="âœ… ç­–åˆ’æ¡ˆå·²ç”Ÿæˆ", state="complete", expanded=False)
            
            st.markdown(report)
            
        except Exception as e:
            st.error(f"å‡ºé”™å•¦: {e}")
